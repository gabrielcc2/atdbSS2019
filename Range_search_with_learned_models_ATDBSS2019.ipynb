{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Range search with learned models@ATDBSS2019.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gabrielcc2/atdbSS2019/blob/master/Range_search_with_learned_models_ATDBSS2019.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P5T5QDLL1UBG",
        "colab_type": "text"
      },
      "source": [
        "# **Range search with learned models**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1T3_N8f17Im",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Welcome everyone.\n",
        "\n",
        "Today we're gonna have a look at how to support range queries with learned models. This notebook is an assignment from the course [\"Advanced Topics in Databases\"](http://www.dbse.ovgu.de/Lehre/ATDB.html), at the University of Magdeburg. Resolving the task should take between 2-3 hours, so plan accordingly.\n",
        "\n",
        "Before jumping into action, two housekeeping notes:\n",
        "\n",
        "1.  For submitting the task 4 items (identified in bold, as **submission items**) are required to be sent to campero@ovgu.de, by 30.06.19, 11:59 pm CET.  \n",
        "2.   If there is any complication or there are questions with this assignment, feel free to reach out to campero@ovgu.de.\n",
        "\n",
        "Traditionally range queries are supported with efficient index structures that allow you to determine the position of the item in the lower bound, the position of the item in the higher bound, and then the positions of items within.\n",
        "\n",
        "An example of a range query could look like this: \n",
        "SELECT \n",
        "    * \n",
        "FROM\n",
        "    lineitem\n",
        "WHERE\n",
        "    l_extendedprice >= 1000 AND l_extendedprice <= 10000;\n",
        "    \n",
        "These kind of queries are a basic building block in data exploration.\n",
        "\n",
        "The idea of supporting this kind of queries with learned models comes from Kraska et al. (https://arxiv.org/abs/1712.01208). These authors had that insight that search can actually be replaced by inference, if there exists a model that maps between the search key and the target position where the item is to be found. This mapping can be ultimately powered by a regressor model.\n",
        "\n",
        "Authors show that other tasks (such as existence queries, and point-lookups) can be supported with learned models too.\n",
        "\n",
        "The use of these models, specially when they are compact, constitutes an elegant adaptation of a data structure to data characteristics. When these models are, in fact, sufficiently compact, the search over these models could be expected to be faster than search over the original data. Hence these are the cornerstone gains from the models: adaptation to the data, reduced storage, improved search. \n",
        "\n",
        "Today we're going to have a quick experiment on building such models, and in making them efficient at inference (i.e., the search task). \n",
        "\n",
        "Please note that the implementation offered for this task is purely illustrative. It is not intended to be representative of the actual state-of-the-art in building these structures.\n",
        "\n",
        "Without further ado, run the following block to install some dependencies, and lets get started."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "90uSdyL7epFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install matplotlib pandas seaborn sklearn keras \n",
        "\n",
        "print(\"Done installing dependencies.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o38bXPSw2MbC",
        "colab_type": "text"
      },
      "source": [
        "# First part: Data understanding\n",
        "\n",
        "(Estimated time: 40 minutes)\n",
        "\n",
        "First, you need to load our dataset. For this task we will be using the lineitem table from a TPC-H database of SF 0.01. From this dataset we will focus on learning a model for range queries on the extended price attribute.\n",
        "\n",
        "Here is the overall schema for the TPC-H database:\n",
        "![](https://docs.snowflake.net/manuals/_images/sample-data-tpch-schema.png)\n",
        "\n",
        "Our data will be loaded from a Github repository. Run the code below to load it into a dataframe. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJLT7RxNJirE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "dataset_url = 'https://raw.githubusercontent.com/gabrielcc2/atdbSS2019/master/data/lineitem_SF0.01.tbl'\n",
        "df = pd.read_csv(dataset_url, sep='|', header=0)\n",
        "\n",
        "# Our dataset is now stored in a Pandas Dataframe\n",
        "\n",
        "#Here we check the schema, and its length\n",
        "print(\"Schema: \"+str(df.columns))\n",
        "print(\"Number of rows: \"+str(len(df)))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYdBKtEy7y8G",
        "colab_type": "text"
      },
      "source": [
        "You will see that you have the same schema as defined in the TPC-H standard, but there is an addition of a column called  *L_EXTENDEDPRICE_SORT_KEY* .\n",
        "\n",
        "This column will come in handy for our task.\n",
        "\n",
        "Remember that our idea is to train a regressor model that maps from the *L_EXTENDEDPRICE*, to a location where items with that given price can be found. For simplicity, lets assume that this location can be the index of an array.\n",
        "\n",
        "If this location were the actual index of the row, it would be very difficult to learn a precise model.\n",
        "\n",
        "Run the code below to get a better sense of this difficulty. Here we plot how the prices (*L_EXTENDEDPRICE*) are distributed, and how they relate to the original index (*ORIGINAL_INDEX*). \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ca0QPX7A9WQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "  \n",
        "\n",
        "df['ORIGINAL_INDEX'] = df.index #For ease of use, we insert a new column with the original positions\n",
        "\n",
        "df_for_plot= df[['L_EXTENDEDPRICE', 'ORIGINAL_INDEX']] #We only keep the columns that we'll use for the plot\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "For zooming-in, feel free to consider only the first k indexes or prices in df2...\n",
        "\n",
        "With the next 2 commands you sort on 'L_EXTENDEDPRICE' values, and retrieve only the first k rows\n",
        "\n",
        "df_for_plot= df_for_plot.sort_values(['L_EXTENDEDPRICE']) #(or alternatively, df_for_plot.sort_values(['ORIGINAL_INDEX']))\n",
        "df_for_plot=df_for_plot[:k]\n",
        "\"\"\"\n",
        "sns.set_palette(\"Paired\")\n",
        "g=sns.pairplot(df_for_plot)\n",
        "g.fig.suptitle(\"Extended Price vs. Original Index\", y=1.08)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpQNaSdf93yM",
        "colab_type": "text"
      },
      "source": [
        "You should see that the *L_EXTENDEDPRICE* goes up to 100k, with apparently few items being on the high end. You'll also see that the original index goes to 50k, with a fair distribution of items in each bin of the histogram.\n",
        "\n",
        "You should also see that there is visible relation (as expected) between *L_EXTENDEDPRICE*  and the *ORIGINAL_INDEX*. This is even more evident in cases where there are duplicate prices (i.e., one price maps to several indexes).\n",
        "\n",
        "Learning a model that maps between prices and these indexes would thus be very challenging. \n",
        "\n",
        "A workaround is to use a *Cummulative Density Function* (CDF) over the spread of the prices. This function gives us a nice 1:1 mapping between price and the CDF, while at the same time being monotonically increasing, and arguably easy to learn with a regressor.\n",
        "\n",
        "In the next code snippet we construct the CDF for this attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_50E6wVZH96s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "#To calculate the actual CDF we need to define the number of bins (i.e., of unique values for the 'L_EXTENDEDPRICE')\n",
        "num_bins = df['L_EXTENDEDPRICE'].nunique()\n",
        "\n",
        "#We create a histogram, counting the number of items per bin, and the boundaries of the bins\n",
        "counts, bin_edges = np.histogram (df['L_EXTENDEDPRICE'], bins=num_bins)\n",
        "prefix_sum = np.cumsum (counts) #This is a cummulative or prefix sum (https://en.wikipedia.org/wiki/Prefix_sum), tracking the sum of the number of items preceeding the list\n",
        "cdf= prefix_sum/prefix_sum[-1]#Dividing by the last value in the list gives us the cdf\n",
        "\n",
        "\n",
        "#Here we use as x the top boundaries of the bins, and as y the cdf \n",
        "df_for_plot = pd.DataFrame(np.array([bin_edges[1:], cdf]).T,\n",
        "                    columns=[\"L_EXTENDEDPRICE\", \"Cummulative Probability\"])\n",
        "sns.set_palette(\"BuGn_r\")\n",
        "g= sns.lineplot(x=\"L_EXTENDEDPRICE\", y=\"Cummulative Probability\", data=df_for_plot)\n",
        "g.set_title(\"CDF for Extended Price\", y=1.08)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrifE_FDObK7",
        "colab_type": "text"
      },
      "source": [
        "Using this CDF, and a random buffer of 1-10 items between values of prices (i.e., perhaps allowing for some data insertion without having to change the model), we added the *'L_EXTENDEDPRICE_SORT_KEY* attribute to our dataset.\n",
        "\n",
        "To understand how this attribute lends itself to a mapping that is easier to learn, lets check how it relates to *L_EXTENDEDPRICE*.\n",
        "\n",
        "Fill-in the following code block and run it, to check this relation. Once the plot is shown, zoom-in to the first 500 values (sorting on *L_EXTENDEDPRICE*) and save it (with any easy to understand filename) as **submission item # 1**. Optionally, you can compare too, in the same plot with the original indexes. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T4nG6FdYKoD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_for_plot= df[[]] #TODO: Fill-in the columns needed for the plot\n",
        "\n",
        "#TODO: Add here the code to zoom-in to the first 500 values (sorting on 'L_EXTENDEDPRICE')\n",
        "sns.set_palette(\"husl\")\n",
        "g= sns.pairplot(df_for_plot)\n",
        "g.fig.suptitle(\"... vs. ...\", y=1.08) #TODO: Fill-in\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpXAOYDL2dZR",
        "colab_type": "text"
      },
      "source": [
        "# Second part:  Model fitting\n",
        "\n",
        "(Estimated time: 80 minutes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WO1No4OpY_0",
        "colab_type": "text"
      },
      "source": [
        "*Regression* is the problem of predicting a target value (commonly a real-value), given an unlabled example. In order to solve this task a regression model is trained on a set of labeled data (e.g. to learn how to predict the weight of a person, based on features), such that the model can then be used to predict the target value of new unlabeled examples.\n",
        "\n",
        "![An example of linear regression](https://scikit-learn.org/stable/_images/sphx_glr_plot_cv_predict_thumb.png)\n",
        "\n",
        "For our case, however, we are not entirely concerned on learning for new data, instead the goal is to minimize the errors of the learned model on the training data itself. As a result, there is no test and train split.\n",
        "\n",
        "In this section of the assignment we will create several models, and compare them. In specific, we will compare:\n",
        "\n",
        "1. A simple linear regressor, against many others (only on the first 2k data points).\n",
        "2. A SVM regressor of your choice, from the Scikit learn models (only on the first 2k data points).\n",
        "3. A naive neural network.\n",
        "4. A hierarhical neural network architecture.\n",
        "\n",
        "In the following snippet we begin by defining in an easy manner our X and Y variables, and the subsets that we will use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DfbQHs34ET9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "from sklearn import preprocessing\n",
        "\n",
        "df=df.sort_values(['L_EXTENDEDPRICE'])\n",
        "df['L_EXTENDEDPRICE_SCALED']=preprocessing.scale(df['L_EXTENDEDPRICE'])\n",
        "\n",
        "X = np.array(df['L_EXTENDEDPRICE']).reshape(-1,1) #A necessary reshape\n",
        "X_subset=X[:2000]#We will train first on a subset of 2k values. \n",
        "\n",
        "X_subset_scaled=np.array(df['L_EXTENDEDPRICE_SCALED']).reshape(-1,1) #A necessary reshape\n",
        "X_subset_scaled=X_subset_scaled[:2000]\n",
        "\n",
        "Y=df['L_EXTENDEDPRICE_SORT_KEY']\n",
        "Y_subset=Y[:2000]#We will train first on a subset of 2k values\n",
        "\n",
        "print(\"Done initializing subsets for training the scikit-learn regressors.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pn7dqT5ws4Gi",
        "colab_type": "text"
      },
      "source": [
        "Next we are going to train several models made available in scikit-learn. Please note that, since many of these models are more suitable for small to mid-sized data, we only evaluate them on a very limited subset of our dataset. Also note that we report the an evaluation on the original data (i.e., we make no train/test splits). Furthermore, we consider here mean absolute error, since it seems easier to understand (for this dataset) than the mean squared error.\n",
        "\n",
        "This step could take some time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfwY6Y-e4q8_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.gaussian_process import GaussianProcessRegressor\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "\"\"\"\n",
        "For each regressor we have a similar workflow:\n",
        "First, we define the regressor\n",
        "Second, we fit the model\n",
        "Third, we predict\n",
        "Fourth, we evaluate the error\n",
        "\"\"\"\n",
        "def max_absolute_error(y, y_pred):\n",
        "    return np.max(np.abs(y - y_pred))\n",
        "\n",
        "max_ae=[]\n",
        "\n",
        "#We begin with linear models, which have the nice feature of having a low number of\n",
        "#coefficients, hence creating models that are compact (i.e., with a low memory footprint)\n",
        "lr = linear_model.LinearRegression(n_jobs=5)#A linear regressor model. The simplest of the lot.\n",
        "lr.fit(X_subset,Y_subset)\n",
        "lr_predict= lr.predict(X_subset)\n",
        "lr_mae=mean_absolute_error(Y_subset, lr_predict)\n",
        "max_ae.append(max_absolute_error(Y_subset, lr_predict))\n",
        "\n",
        "lasso = linear_model.Lasso(alpha=1.0) #A linear lasso model (least absolute shrinkage and selection operator)\n",
        "#In addition to regression, Lasso models are also commonly employed for feature selection.\n",
        "lasso.fit(X_subset, Y_subset)\n",
        "lasso_predict=lasso.predict(X_subset)\n",
        "lasso_mae=mean_absolute_error(Y_subset, lasso_predict)\n",
        "max_ae.append(max_absolute_error(Y_subset, lasso_predict))\n",
        "\n",
        "ts= linear_model.TheilSenRegressor(random_state=0) #A Theil-Sen model, a regressor based on an estimation of the\n",
        "#geometrical median of the slopes of all possible lines connecting pairs of observations in the data\n",
        "ts.fit(X_subset, Y_subset)\n",
        "ts_predict=ts.predict(X_subset)\n",
        "ts_mae=mean_absolute_error(Y_subset, ts_predict)\n",
        "max_ae.append(max_absolute_error(Y_subset, ts_predict))\n",
        "\n",
        "\n",
        "#We also consider a kernel ridge model. For this approach it is necessary to normalize the data on the X axis.\n",
        "\"\"\"This, unfortunately does not learn a compact model, but stores a lot of data\n",
        "which is also used to make the predictions.\"\"\"\n",
        "kr = KernelRidge(alpha=0.1, kernel='poly')#A kernel ridge model with a linear kernel by default\n",
        "kr.fit(X_subset_scaled, Y_subset) \n",
        "kr_predict=kr.predict(X_subset_scaled)\n",
        "kr_mae=mean_absolute_error(Y_subset, kr_predict)\n",
        "max_ae.append(max_absolute_error(Y_subset, kr_predict))\n",
        "\n",
        "#We also consider a Gaussian process model\n",
        "\"\"\"This, unfortunately has a large memory footprint.\"\"\"\n",
        "gpr = GaussianProcessRegressor(random_state=0)#A gaussian process model\n",
        "gpr.fit(X_subset, Y_subset)\n",
        "gpr_predict=gpr.predict(X_subset)\n",
        "gpr_mae=mean_absolute_error(Y_subset, gpr_predict)\n",
        "max_ae.append(max_absolute_error(Y_subset, gpr_predict))\n",
        "\n",
        "#We also consider an AdaBoost model\n",
        "ab= AdaBoostRegressor(random_state=0, n_estimators=100)\n",
        "ab.fit(X_subset, Y_subset)  \n",
        "ab_predict=ab.predict(X_subset)\n",
        "ab_mae=mean_absolute_error(Y_subset, ab_predict)\n",
        "max_ae.append(max_absolute_error(Y_subset, ab_predict))\n",
        "\n",
        "#We also consider a Decision Tree model\n",
        "dt = DecisionTreeRegressor(random_state=0)\n",
        "dt.fit(X_subset, Y_subset)\n",
        "dt_predict=dt.predict(X_subset)\n",
        "dt_mae=mean_absolute_error(Y_subset, dt_predict)\n",
        "max_ae.append(max_absolute_error(Y_subset, dt_predict))\n",
        "\n",
        "#TODO: Add here an SVR model... don't forget to also add its max absolute error value to max_ae\n",
        "\n",
        "\n",
        "mean_absolute_error_regressors=[lr_mae, lasso_mae, ts_mae, kr_mae, gpr_mae, ab_mae, dt_mae]\n",
        "labels_regressors=[\"LR\", \"Lasso\", \"TS\", \"KR\", \"GP\", \"AB\", \"DT\"]\n",
        "summary = dict()\n",
        "for i in range(0,len(labels_regressors)):\n",
        "  summary[labels_regressors[i]]=dict()\n",
        "  summary[labels_regressors[i]][\"MAE\"]=mean_absolute_error_regressors[i]\n",
        "  summary[labels_regressors[i]][\"MaxAE\"]=max_ae[i]\n",
        "g = sns.barplot(x=labels_regressors, y=mean_absolute_error_regressors)\n",
        "g.set_title(\"Mean absolute error for different models\", y=1.08)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KliLNH1qJVhG",
        "colab_type": "text"
      },
      "source": [
        "For your **submission item #2** extend the former code box with one Support Vector Regressor model. Here are examples: [SVR@sklearn](https://scikit-learn.org/stable/auto_examples/svm/plot_svm_regression.html). Plot the results of this model  after the results of DT, in the former code box, and save the plot to a file, for your submission. Add to the filename, or in a note, the basic details about your configuration (e.g. SVR_deg3_gamma_auto.png). For this method, just like for Kernel Ridge regressor, it is good practice to use X_scaled (otherwise it takes a lot of time to run, and the results are deteriorated).\n",
        "\n",
        "Now lets take a look at how the predictions actually match for the models that you have.\n",
        "\n",
        "First we do this on a subset of the first 10 values. For your **submission item #3**, produce and download the plot for the first 1000 values, and observe better how linear models give a reasonable approximation (though without visually seeming to improve over GP and DT). This might take some minutes to run.\n",
        "\n",
        "If the SVR was not possible to implement, still submit the results for the item #3 without this model.\n",
        "\n",
        "If not vissible, remember that DT and GP are currently equal to the target function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pc4JrjFUr5YZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO: Change to show 1000 values. If needed, also add the SVR predictions.\n",
        "\n",
        "X_plot=df['L_EXTENDEDPRICE']\n",
        "x_axis= X_plot[:10]\n",
        "sns.set_palette(\"bright\")\n",
        "g= sns.lineplot(x=x_axis, y=Y_subset[:10], label=\"Target\")\n",
        "g= sns.lineplot(ax=g, x=x_axis, y=lr_predict[:10], label=\"LR Predictions\")\n",
        "g= sns.lineplot(ax=g, x=x_axis, y=lasso_predict[:10], label=\"Lasso Predictions\")\n",
        "g= sns.lineplot(ax=g, x=x_axis, y=ts_predict[:10], label=\"TheilSen Predictions\")\n",
        "g= sns.lineplot(ax=g, x=x_axis, y=kr_predict[:10], label=\"Kernel Ridge Predictions\")\n",
        "g= sns.lineplot(ax=g, x=x_axis, y=gpr_predict[:10], label=\"Gaussian Process Predictions\")\n",
        "g= sns.lineplot(ax=g, x=x_axis, y=ab_predict[:10], label=\"AdaBoost Predictions\")\n",
        "g= sns.lineplot(ax=g, x=x_axis, y=dt_predict[:10], label=\"Decision Tree Predictions\")\n",
        "\n",
        "g.set_title(\"Extended Price Sort Key (for first 10 entries), compared to predictions\", y=1.08)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkABnBpZ1bcS",
        "colab_type": "text"
      },
      "source": [
        "We see non-linear models perform well, but there is an overhead in their memory footprint. Lets evaluate that. To estimate the memory usage we serialize the models, using the pickle library. \n",
        "\n",
        "Optionally, add to this plot (and send) the SVR model's footprint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JR78yQvr1lM-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO: If possible, add the SVR model.\n",
        "\n",
        "import pickle\n",
        "model_names=[\"Orig. data\", \"LR\", \"Lasso\", \"TS\", \"KR\", \"GP\", \"AB\", \"DT\"]\n",
        "memory_footprint_of_models= [\n",
        "    len(pickle.dumps(Y_subset))+len(pickle.dumps(X_subset)),\n",
        "    len(pickle.dumps(lr)), \n",
        "    len(pickle.dumps(lasso)), \n",
        "    len(pickle.dumps(ts)), \n",
        "    len(pickle.dumps(kr)),\n",
        "    len(pickle.dumps(gpr)),\n",
        "    len(pickle.dumps(ab)),\n",
        "    len(pickle.dumps(dt))]\n",
        "\n",
        "for i in range(0,len(labels_regressors)):\n",
        "  summary[labels_regressors[i]][\"Memory (bytes)\"]=memory_footprint_of_models[i+1]\n",
        "  \n",
        "g = sns.barplot(x=model_names, \n",
        "    y=memory_footprint_of_models,palette=sns.xkcd_palette([\"windows blue\", \"amber\", \"greyish\", \"faded green\", \n",
        "                                  \"dusty purple\"]))\n",
        "g.set_yscale('log')\n",
        "g.set_title(\"Estimated memory footprint (in bytes) of the models\", y=1.08)\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Jd22p6VQQar",
        "colab_type": "text"
      },
      "source": [
        "The memory footprint plot shows that all linear models are notably smaller in their memory usage, when compared to the original data. Models from KR onwards are closer to (or larger) than the size of the original data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icN1PR7b1l3i",
        "colab_type": "text"
      },
      "source": [
        "**Bonus:** An open question all the time, when testing a model, is: What is the impact of hyper-parameters? Or stated differently, how can we be sure that we picked the best model configuration?\n",
        "\n",
        "In the following code snippet is an example of how, in practice, grid search is done, to find the best set of hyper-parameters for a given model.\n",
        "\n",
        "Please note that the scores will not match those of the algorithm in the previous sections, since the grid search evaluates on splits of test-train. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EVifvich6jBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "#Since this throws some warnings, lets not consider them...\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "parameters = {'kernel':('linear', 'rbf', 'poly'), 'alpha':[0.1, 1.0]}#Other parameters could be included\n",
        "base_model = KernelRidge()\n",
        "grid = GridSearchCV(base_model, parameters, cv=2, scoring=make_scorer(mean_absolute_error, greater_is_better=False))\n",
        "grid.fit(X_subset_scaled, Y_subset)\n",
        "\n",
        "print(\"Best parameter set found:\")\n",
        "print(grid.best_params_)\n",
        "\n",
        "print(\"Grid scores found:\")\n",
        "means = grid.cv_results_['mean_test_score']\n",
        "stds = grid.cv_results_['std_test_score']\n",
        "for mean, std, params in zip(means, stds, grid.cv_results_['params']):\n",
        "  print(\"%0.3f (+/-%0.03f) for %r\"% (mean, std * 2, params))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP-H0jVgpotA",
        "colab_type": "text"
      },
      "source": [
        "Next, we'll start with a neural network model, which forms a good trade-off between memory footprint and the behavior in the regression task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxHJc-qqYFhI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import regularizers\n",
        "\n",
        "print(\"Done importing some necessary dependencies.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5x-Jvz4rI9A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We attempt first a naive implementation that considers all of the data\n",
        "\n",
        "def NN_regressor():\n",
        "  model= Sequential()\n",
        "  model.add(Dense(128,input_dim=1, kernel_initializer='normal', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
        "  model.add(Dense(1,input_dim=128, kernel_initializer='normal', activation='linear'))\n",
        "  model.compile(loss='mean_absolute_error', optimizer='adam')\n",
        "  return model\n",
        "\n",
        "naive_model = NN_regressor()\n",
        "naive_model.fit(x=X, y=Y, epochs=10,steps_per_epoch=20,verbose=1)\n",
        "\n",
        "print(\"Done with fitting a naive model\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbnXL76EX9m2",
        "colab_type": "text"
      },
      "source": [
        "As can be seen, training such models over the whole dataset will require some effort. Given the size of the dataset, it would be easier to break the learning of the model into a hierarchical solution: At the top a multi-class classifier, deciding which model from underneath would answer to the query. At the bottom, a series of regressors learning on a given subset of data.\n",
        "\n",
        "This smart idea was suggested by Kraska et al., calling such ensemble a Recursive Index Model. ![alt text](https://adriancolyer.files.wordpress.com/2018/01/learned-index-fig-3.jpeg?w=520)\n",
        "\n",
        "In the next task, lets build and train one such model consisting of one parent, and 4 regressor nodes as children. \n",
        "\n",
        "To this end we need to start by creating a mapping between the L_EXTENDEDPRICE and 4 class\n",
        "labels corresponding to the children nodes.\n",
        "\n",
        "Here we will assign labels 0,1,2,3 by breaking the data points based on the quantiles.\n",
        "To learn about the parameters and configurations available for building networks with Keras, check their documentation: [Keras](https://keras.io/).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tQo_v6dd3v6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "price_quantiles=[\n",
        "    np.quantile(df['L_EXTENDEDPRICE'], .25),\n",
        "    np.quantile(df['L_EXTENDEDPRICE'], .50),\n",
        "    np.quantile(df['L_EXTENDEDPRICE'], .75)]\n",
        "\n",
        "df['classifier_labels']=[0 if x<=price_quantiles[0] \n",
        "                      else \n",
        "                             1 if x<=price_quantiles[1]\n",
        "                      else\n",
        "                             2 if x<=price_quantiles[2]\n",
        "                      else\n",
        "                             3 for x in df['L_EXTENDEDPRICE']]\n",
        "\n",
        "print(\"Done, with assigning labels to items based on quantiles.\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3M9f98Z1OBm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn import preprocessing\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "df = shuffle(df) \n",
        "\n",
        "#Some house keeping, encoding the labels\n",
        "encoder = preprocessing.LabelEncoder()\n",
        "encoder.fit(df['classifier_labels'])\n",
        "encoded_Y = encoder.transform(df['classifier_labels'])\n",
        "# We convert integers to dummy variables (i.e. one hot encoded)\n",
        "dummy_y = to_categorical(encoded_Y)\n",
        "\n",
        "print(\"Done with some prep to train the recursive model index parent model (the classifier).\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mgYmMWbuYEj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here we define and train a multi-class classifier mapping to 4 classes\n",
        "def NN_classifier():\n",
        "  model= Sequential()\n",
        "  model.add(Dense(12,input_dim=1, kernel_initializer='uniform', activation='relu'))\n",
        "  model.add(Dense(8,input_dim=12, kernel_initializer='uniform', activation='relu'))\n",
        "  model.add(Dense(output_dim=4,input_dim=8, kernel_initializer='uniform', activation='softmax'))\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "  return model\n",
        "\n",
        "#In the following we train a model that should have a classification accuracy higher than 90%\n",
        "parent_model = NN_classifier()\n",
        "parent_model.fit(x=df['L_EXTENDEDPRICE'], y=dummy_y, epochs=30,steps_per_epoch=200,verbose=1)\n",
        "\n",
        "print(\"Done with fitting the parent model.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIJCPXEriPJ_",
        "colab_type": "text"
      },
      "source": [
        "Now we have built a classifier. It is time to evaluate it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUX9FJj39fR2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here we load a model that corresponds to a pre-trained classifier (just to make sure we have a very accurate model for the next steps).\n",
        "\n",
        "from keras.models import load_model\n",
        "from urllib.request import urlopen\n",
        "remote_model_data = urlopen('https://github.com/gabrielcc2/atdbSS2019/raw/master/models/parent_model.h5').read()\n",
        "f = open('parent_model.h5', 'wb')\n",
        "f.write(remote_model_data)\n",
        "parent_model = load_model('parent_model.h5')\n",
        "\n",
        "print(\"Done with loading a pre-trained parent model\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7npBMJ5y7x3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Lets check the accuracy of the model we just loaded (this might take some time)\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "parent_model_preds=[np.argmax(parent_model.predict([x], batch_size=1)) for x in df['L_EXTENDEDPRICE']] #Here the predictions are the positions that ranked higher from the one hot encoding.\n",
        "\n",
        "print(\"Accuracy score for pre-trained classifier: \"+str(accuracy_score(df['classifier_labels'], parent_model_preds)))\n",
        "\n",
        "#Now we replace the original labels with those given by the predictor.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD2-egn2iZiI",
        "colab_type": "text"
      },
      "source": [
        "After building and evaluating the parent model (i.e., the classifier), we can train the leaf nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zdhLSVn9GgWP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#We will use the predictions of the parent model to distribute the data to child nodes when training.\n",
        "\n",
        "df['new_classifier_labels']=parent_model_preds\n",
        "\n",
        "# The following line could work for checking that all items in X_subset are for the first regressor\n",
        "#print([np.argmax(parent_model.predict([x], batch_size=1)) for x in X_subset])\n",
        "\n",
        "print(\"We stored the predictions of the parent model, to use them as our new labels.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hgp1fe2lCRYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models=[]\n",
        "print(\"We created an empty models array, to store the leaf nodes from the recursive model index.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yh6LwGQGLX2Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Now we are going to train the four child regressor models\n",
        "#This will take some time. You could try changing the number of epochs, to train less.\n",
        "\n",
        "def NN_regressor():\n",
        "  model= Sequential()\n",
        "  model.add(Dense(128,input_dim=1, kernel_initializer='he_uniform', activation='relu'))\n",
        "  model.add(Dense(64,input_dim=128, kernel_initializer='he_uniform', activation='relu'))\n",
        "  model.add(Dense(32,input_dim=64, kernel_initializer='he_uniform', activation='relu'))\n",
        "  model.add(Dense(64,input_dim=32, kernel_initializer='he_uniform', activation='relu'))\n",
        "  model.add(Dense(128,input_dim=64, kernel_initializer='he_uniform', activation='relu'))\n",
        "  model.add(Dense(1,input_dim=128, kernel_initializer='he_uniform', activation='relu'))\n",
        "  model.compile(loss='mean_squared_error', optimizer='adamax', metrics=['mae'])\n",
        "  return model\n",
        "\n",
        "\n",
        "for item in range(0,4):\n",
        "  if (len(models)==item):\n",
        "    models.append(NN_regressor())\n",
        "  else:\n",
        "    models[item]=NN_regressor()\n",
        "  df_subset=df[df['new_classifier_labels']==item]\n",
        "  models[item].fit(x=df_subset['L_EXTENDEDPRICE_SCALED'], y=df_subset['L_EXTENDEDPRICE_SORT_KEY'], epochs=20, steps_per_epoch=50, verbose=1)\n",
        "  models[item].save(\"reg_model_\"+str(item)+\".h5\")\n",
        "  \n",
        "print(\"Great! We created and trained our 4 regressors.\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFqlh1R8qYTz",
        "colab_type": "text"
      },
      "source": [
        "That was quite a good job! Now we have one recursive model index.\n",
        "\n",
        "Please note that the architectures of the parent and leave nodes are not guaranteed to be the best possible ones given our data and the learning task; however they are sufficient to have an end-to-end index.\n",
        "\n",
        "For those interested in automating network search, here are some references of ongoing projects: [AutoKeras](https://github.com/keras-team/autokeras), [Neural Architecture Search with Reinforcement Learning](https://ai.google/research/pubs/pub45826) and [NAS Bench](https://github.com/google-research/nasbench).\n",
        "\n",
        "Lets consider next the overall mean absolute error, and the maximum error of our RMI model. To this end, in the next code box, we'll load some pre-trained models for the regressors. You can alternatively skip to the following code box and test your local models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cifs0NcbHpBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "models=[]\n",
        "for item in range(0,4):\n",
        "  remote_model_data = urlopen('https://github.com/gabrielcc2/atdbSS2019/raw/master/models/'+\"reg_model_\"+str(item)+\".h5\").read()\n",
        "  f = open(\"reg_model_\"+str(item)+\".h5\", 'wb')\n",
        "  f.write(remote_model_data)\n",
        "  models.append(load_model(\"reg_model_\"+str(item)+\".h5\"))\n",
        "\n",
        "  print(\"Done fetching remote pre-trained models\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H2Xx3K_QcaYl",
        "colab_type": "text"
      },
      "source": [
        "This might take some time to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTpESy5HZT1c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_absolute_error(y, y_pred):\n",
        "    return np.max(np.abs(y - y_pred))\n",
        "\n",
        "mean_absolute_errors=[]\n",
        "max_absolute_errors=[]\n",
        "max_error=0\n",
        "for item in range(0,4):\n",
        "  df_subset=df[df['new_classifier_labels']==item]\n",
        "  x_=df_subset['L_EXTENDEDPRICE_SCALED']\n",
        "  y_=df_subset['L_EXTENDEDPRICE_SORT_KEY']\n",
        "  preds=[models[item].predict([x], batch_size=1)[0] for x in x_]\n",
        "  mean_absolute_errors.append(mean_absolute_error(y_, preds))\n",
        "  preds=np.array(preds)\n",
        "  preds=preds.reshape(np.shape(preds)[0])\n",
        "  error= max_absolute_error(y_, preds)\n",
        "  max_absolute_errors.append(error)\n",
        "  if error>max_error:\n",
        "    max_error=error\n",
        "\n",
        "print(\"(Drumrolls please...)\")\n",
        "print(\"****Results for the Recursive Model Index (on the complete data)****\")\n",
        "print(\"Mean absolute error per model: \")\n",
        "print(mean_absolute_errors)\n",
        "print(\"Mean absolute error: \"+str(np.mean(mean_absolute_errors)))\n",
        "print(\"Max absolute error per model: \")\n",
        "print(max_absolute_errors)\n",
        "print(\"Max error: \"+str(max_error))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0xDOFD7h5mM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here we calculate the errors on X_subset, such that we can compare it with the regressors we trained at the start\n",
        "max_error=0\n",
        "preds=[models[0].predict([x], batch_size=1)[0] for x in X_subset_scaled]\n",
        "rmi_mae = mean_absolute_error(Y_subset, preds)\n",
        "for i in range(0,len(Y_subset)):\n",
        "  if Y_subset.iloc[i]>preds[i]:\n",
        "    error= Y_subset.iloc[i]-preds[i]\n",
        "  else:\n",
        "    error= preds[i]-Y_subset.iloc[i]\n",
        "  if error>max_error:\n",
        "    max_error=error[0]\n",
        "\n",
        "summary[\"RMI\"]=dict()\n",
        "summary[\"RMI\"][\"MAE\"]=rmi_mae\n",
        "summary[\"RMI\"][\"MaxAE\"]=max_error\n",
        "\n",
        "print(\"Mean absolute error for RMI: \"+str(rmi_mae))\n",
        "print(\"Max error for RMI: \"+str(max_error))\n",
        "\n",
        "\n",
        "sns.set_palette(\"Set2\")\n",
        "names=list(summary.keys())\n",
        "g = sns.barplot(x=names, y=[summary[key][\"MAE\"] for key in names])\n",
        "g.set_title(\"Mean absolute error for different models\", y=1.08)\n",
        "plt.show()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WX_j0EFPQxJ",
        "colab_type": "text"
      },
      "source": [
        "RMI should not be doing that bad in this subset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otOrCXivRyY1",
        "colab_type": "text"
      },
      "source": [
        "Pack the previous plot and the following plot, these are your **submission items #4 and #5**. With this you conclude the assignment, but you are welcome to check the extra task, to learn more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fa6vXUlcmUDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model_names=[\"Orig. data\", \"LR\", \"Lasso\", \"TS\", \"KR\", \"GP\", \"AB\", \"DT\"]\n",
        "memory_footprint_of_models= [\n",
        "    len(pickle.dumps(Y_subset))+len(pickle.dumps(X_subset)),\n",
        "    len(pickle.dumps(lr)), \n",
        "    len(pickle.dumps(lasso)), \n",
        "    len(pickle.dumps(ts)), \n",
        "    len(pickle.dumps(kr)),\n",
        "    len(pickle.dumps(gpr)),\n",
        "    len(pickle.dumps(ab)),\n",
        "    len(pickle.dumps(dt))]\n",
        "\n",
        "model_names.append(\"RMI\")\n",
        "memory_footprint_of_models.append(len(pickle.dumps(models[0]))+len(pickle.dumps(parent_model)))\n",
        "\n",
        "model_names.append(\"RMI_w\")\n",
        "memory_footprint_of_models.append(len(pickle.dumps(models[0].get_weights()))+len(pickle.dumps(parent_model.get_weights())))\n",
        "summary[\"RMI\"][\"Memory (bytes)\"]=len(pickle.dumps(models[0].get_weights()))+len(pickle.dumps(parent_model.get_weights()))\n",
        "\n",
        "g = sns.barplot(x=model_names, \n",
        "    y=memory_footprint_of_models,palette=sns.xkcd_palette([\"windows blue\", \"amber\", \"greyish\", \"faded green\", \n",
        "                                  \"dusty purple\"]))\n",
        "g.set_yscale('log')\n",
        "g.set_title(\"Estimated memory footprint (in bytes) of the models\", y=1.08)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGrNcAzJoD8O",
        "colab_type": "text"
      },
      "source": [
        "As a result, we see that the recursive model index we built, using neural networks represents a good solution for having a low mean absolute error, and a limited memory footprint, when compared to other models. \n",
        "\n",
        "In our implementation, depending on our choice of neural architecture, it might be that Decision Trees seem to be better solutions in the trade-off between memory efficiency and mean average error.\n",
        "\n",
        "Smaller network structures could also be employed, reducing the footprint of the RMIs. This approach is also comparatively promising when considering only the weights that need to be kept (as RMI_w in the plot).\n",
        "\n",
        "**This section concludes the programming assignment. **\n",
        "\n",
        "In an extra section you can (optionally) evaluate the structure that we built, considering how it performs in the search task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWfbd8S7MMn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"To wrap things up, let's make a summary, to get a better sense of the trade-offs.\n",
        "\"\"\"\n",
        "print(pd.DataFrame.from_dict(summary).transpose())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htGx2u2T4gdR",
        "colab_type": "text"
      },
      "source": [
        "# Takeaways\n",
        "\n",
        "Kudos! You made it to the end of this programming assignment :)\n",
        "\n",
        "For students of the \"Advanced Topics in Databases\" course, please pack in a zip file **submission items #1-#5**, and send them to: campero@ovgu.de. This constitutes your submission, to be evaluated. The deadline is 30.06.19, 11:59 pm CET. Please add some feedback to the submission email, answering at least to the following questions: \n",
        "1.   On a scale from 1-5 (great to terrible), did you find this assignment to be relevant in developing skills useful for your professional path? \n",
        "2.  How would you rate the difficulty of the assignment, on a 1-5 scale (from very easy to very difficult)?\n",
        "3. What suggestions for improvement would you like us to consider?\n",
        "\n",
        "By the end of this assignment you should:\n",
        "\n",
        "1. Understand some of the key ideas for using *learned models* to solve everyday computing tasks: the role of the cummulative distribution function and how recursive model indexes can be built.\n",
        "2.  Be more familiarized with colab, pandas, seaborn, sklearn and keras, as a set of tools for building models and data exploration.\n",
        "3. Be encouraged to think on machine learning models with a practical perspective, considering their memory footprint and details that affect their execution time.\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1AjWDi6cnpxY21WExPRa7ltXmzts7RvHs)\n",
        "\n",
        "If you successfully completed this task, you found it to be enjoyable & would like to collaborate in researching applications of AI techniques in data management, please contact: campero@ovgu.de."
"\n",
"\n",
"If you would like to know more about the challenges for building automatically this kind of index structures, in a real-world scenarion, and managing ML models in production, here is a good reference on the monitoring and testing tasks that are required: Breck, Eric, Shanqing Cai, Eric Nielsen, Michael Salib, and D. Sculley. "The ml test score: A rubric for ml production readiness and technical debt reduction." In 2017 IEEE International Conference on Big Data (Big Data), pp. 1123-1132. IEEE, 2017\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftN-vMxb4Omm",
        "colab_type": "text"
      },
      "source": [
        "# Extra part (Optional): Efficient Inference\n",
        "\n",
        "(Estimated time: 40 minutes)\n",
        "\n",
        "To be added, stay tuned."
      ]
    }
  ]
}
